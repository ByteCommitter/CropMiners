{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7897b972",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2464bd0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'crop_recommendation.csv')\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d33e0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values=[\"N/A\",np.nan,\"na\"]\n",
    "df = pd.read_csv(r'crop_recommendation.csv',na_values=missing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75216814",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efdbd5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce5bdcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,12))\n",
    "i=1\n",
    "for col in df.iloc[:,:-1]:\n",
    "    plt.subplot(3,3,i)\n",
    "    df[[col]].boxplot()\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ec1d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Q1, Q3, and IQR for all columns except the last one\n",
    "Q1 = df.iloc[:, :-1].quantile(0.25)\n",
    "Q3 = df.iloc[:, :-1].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Define a mask for values that are NOT outliers\n",
    "mask = ~((df.iloc[:, :-1] < (Q1 - 1.5 * IQR)) | (df.iloc[:, :-1] > (Q3 + 1.5 * IQR)))\n",
    "\n",
    "# Apply the mask to df, keeping all rows in the last column\n",
    "df_no_outliers = df[mask.all(axis=1)]\n",
    "df_no_outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0c26ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Separate the features from the labels\n",
    "features = df_no_outliers.iloc[:, :-1]\n",
    "labels = df_no_outliers.iloc[:, -1]\n",
    "\n",
    "# Create the scaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit the scaler to the features and transform\n",
    "scaled_features = scaler.fit_transform(features)\n",
    "\n",
    "# Convert the scaled features back into a DataFrame\n",
    "df_scaled = pd.DataFrame(scaled_features, columns=features.columns)\n",
    "\n",
    "# Add the labels back into the DataFrame\n",
    "df_scaled['label'] = labels.values\n",
    "df_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c96a812",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a8ef2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Separate the features from the labels\n",
    "features = df_scaled.iloc[:, :-1]\n",
    "labels = df_scaled.iloc[:, -1]\n",
    "\n",
    "# Initialize a random forest classifier\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "# Fit the model to the data\n",
    "model.fit(features, labels)\n",
    "\n",
    "# Get the feature importances\n",
    "importances = model.feature_importances_\n",
    "\n",
    "# Get the feature names\n",
    "feature_names = features.columns\n",
    "\n",
    "# Combine the feature names and importances into a dictionary\n",
    "feature_importances = dict(zip(feature_names, importances))\n",
    "\n",
    "# Print the feature importances\n",
    "for feature, importance in feature_importances.items():\n",
    "    print(f\"{feature}: {importance * 100 }% \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c7e8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Initialize a PCA\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "# Separate the features from the labels\n",
    "features_scaled = df_scaled.iloc[:, :-1]\n",
    "labels = df_scaled.iloc[:, -1]\n",
    "\n",
    "# Perform PCA on the scaled features\n",
    "pca_features = pca.fit_transform(features_scaled)\n",
    "\n",
    "# Create a new DataFrame for the PCA features\n",
    "df_pca_scaled = pd.DataFrame(data = pca_features, columns = ['principal component 1', 'principal component 2'])\n",
    "\n",
    "# Add the labels to the DataFrame\n",
    "df_pca_scaled['label'] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab58666f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a scatter plot of the two principal components\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(df_pca_scaled['principal component 1'], df_pca_scaled['principal component 2'], c='blue')\n",
    "\n",
    "# Set the title and labels of the plot\n",
    "plt.title('Principal Component Analysis')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b0d00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculate the IQR of each column\n",
    "Q1 = df_pca_scaled.iloc[:, :-1].quantile(0.25)\n",
    "Q3 = df_pca_scaled.iloc[:, :-1].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Define the upper and lower bounds for outliers\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "# Remove the outliers\n",
    "df_pca_no_outliers = df_pca_scaled[~((df_pca_scaled.iloc[:, :-1] < lower_bound) | (df_pca_scaled.iloc[:, :-1] > upper_bound)).any(axis=1)]\n",
    "\n",
    "print(\"Data after removing outliers:\")\n",
    "print(df_pca_no_outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e2aeee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a scatter plot of the two principal components\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(df_pca_no_outliers['principal component 1'], df_pca_no_outliers['principal component 2'], c='blue')\n",
    "\n",
    "# Set the title and labels of the plot\n",
    "plt.title('Principal Component Analysis')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84386fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46492f47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3eb6bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70dd49e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the unique labels\n",
    "unique_labels = df_scaled['label'].unique()\n",
    "\n",
    "# Your existing code...\n",
    "pca_dfs = {}\n",
    "\n",
    "# Loop over the unique labels\n",
    "for label in unique_labels:\n",
    "    # Subset the DataFrame for the current label\n",
    "    subset = df_scaled[df_scaled['label'] == label].iloc[:, :-1]\n",
    "    \n",
    "    # Perform PCA on the subset\n",
    "    pca_features = pca.fit_transform(subset)\n",
    "    \n",
    "    # Create a new DataFrame for the PCA features\n",
    "    df_pca = pd.DataFrame(data = pca_features, columns = ['PC1', 'PC2'])\n",
    "    \n",
    "    # Add the labels to the DataFrame\n",
    "    df_pca['label'] = label\n",
    "    \n",
    "    # Store the DataFrame in the dictionary\n",
    "    pca_dfs[label] = df_pca\n",
    "    \n",
    "    # Create a scatter plot of the PCA features\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    plt.scatter(df_pca['PC1'], df_pca['PC2'])\n",
    "    \n",
    "    # Add labels and title\n",
    "    plt.xlabel('Principal Component 1')\n",
    "    plt.ylabel('Principal Component 2')\n",
    "    plt.title(f'PCA of {label} Data')\n",
    "    \n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1050c05e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2747ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over the unique labels\n",
    "for label in unique_labels:\n",
    "    # Subset the DataFrame for the current label\n",
    "    subset = df_scaled[df_scaled['label'] == label].iloc[:, :-1]\n",
    "    \n",
    "    # Perform PCA on the subset\n",
    "    pca.fit(subset)\n",
    "    \n",
    "    # Get the feature importance for each Principal Component\n",
    "    feature_importance = pd.DataFrame(pca.components_, columns=subset.columns, index=['PC1', 'PC2'])\n",
    "    \n",
    "    # Print the feature importance\n",
    "    print(f\"Feature importance for {label} data:\")\n",
    "    print(feature_importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c173f0",
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81745b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "# Load the data\n",
    "data = pd.read_csv('crop_recommendation.csv')\n",
    "\n",
    "# Assume 'label' is the target variable and the rest are features\n",
    "X = data.drop('label', axis=1)\n",
    "y = data['label']\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a GBM classifier\n",
    "gbm = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=42)\n",
    "\n",
    "# Train the classifier\n",
    "gbm.fit(X_train, y_train)\n",
    "\n",
    "# Use the trained classifier to make predictions on the test set\n",
    "predictions = gbm.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f45befc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Calculate the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f\"Model Accuracy: {accuracy}\")\n",
    "\n",
    "# Print the classification report\n",
    "report = classification_report(y_test, predictions)\n",
    "print(f\"Classification Report: \\n{report}\")\n",
    "\n",
    "# Print the confusion matrix\n",
    "matrix = confusion_matrix(y_test, predictions)\n",
    "print(f\"Confusion Matrix: \\n{matrix}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efed3e03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
